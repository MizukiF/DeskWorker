import cv2
import depthai as dai
import blobconverter
import numpy as np

pipeline = dai.Pipeline()

# Load models 
detection_model_path = blobconverter.from_zoo(
    name="face-detection-retail-0005",
    zoo_type="intel",
    shaves=6
)

pose_model_path = blobconverter.from_zoo(
    name="head-pose-estimation-adas-0001",
    zoo_type="intel",
    shaves=6
)

MIN_THRESHOLD = 15  # yaw/pitch/roll threshold

# ğŸ¥ 1ï¸âƒ£ ã‚«ãƒ¡ãƒ©ãƒãƒ¼ãƒ‰
cam = pipeline.createColorCamera()
cam.setPreviewSize(300, 300)  # ğŸ”¥ 600Ã—600 â†’ 300Ã—300 ã«ä¿®æ­£
cam.setInterleaved(False)
cam.setFps(30)

# ğŸ·ï¸ 2ï¸âƒ£ é¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«
detection_nn = pipeline.createNeuralNetwork()
detection_nn.setBlobPath(detection_model_path)

# ğŸ”³ 3ï¸âƒ£ ImageManip - é¡”ã®ã‚¯ãƒ­ãƒƒãƒ—ã¨ãƒªã‚µã‚¤ã‚º (60x60)
pose_manip = pipeline.createImageManip()
pose_manip.inputConfig.setWaitForMessage(True)  # ğŸ”¥ ä¿®æ­£
pose_manip.initialConfig.setKeepAspectRatio(True)  # ğŸ”¥ ãƒªã‚µã‚¤ã‚ºã¯å¾Œã§è¨­å®š

# ğŸ¯ 4ï¸âƒ£ å§¿å‹¢æ¨å®šãƒ¢ãƒ‡ãƒ«
pose_nn = pipeline.createNeuralNetwork()
pose_nn.setBlobPath(pose_model_path)

# ğŸ“¡ 5ï¸âƒ£ Output nodes
xout_rgb = pipeline.createXLinkOut()
xout_rgb.setStreamName("rgb")
cam.preview.link(xout_rgb.input)

xout_face = pipeline.createXLinkOut()
xout_face.setStreamName("face_detection")

xout_pose = pipeline.createXLinkOut()
xout_pose.setStreamName("pose_estimation")

# ğŸ”¥ 6ï¸âƒ£ pose_manip ã« XLinkIn ã‚’è¿½åŠ 
xin_pose_cfg = pipeline.createXLinkIn()
xin_pose_cfg.setStreamName("pose_manip_config")
xin_pose_cfg.out.link(pose_manip.inputConfig)

# ğŸ“Œ **ä¿®æ­£ç‚¹:** é¡”ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ `pose_manip` ã«æ¸¡ã—ã€60Ã—60 ã«ãƒªã‚µã‚¤ã‚º
cam.preview.link(detection_nn.input)  # ã‚«ãƒ¡ãƒ© â†’ é¡”æ¤œå‡º
detection_nn.out.link(xout_face.input)  # é¡”æ¤œå‡ºçµæœã‚’å‡ºåŠ›

# ğŸ”¥ **é¡”æ¤œå‡ºã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ `pose_manip` ã«é€ã‚‹**
cam.preview.link(pose_manip.inputImage)

# ğŸ”¥ **ã‚¯ãƒ­ãƒƒãƒ— & 60Ã—60 ã«ãƒªã‚µã‚¤ã‚ºã—ãŸé¡”ç”»åƒã‚’å§¿å‹¢æ¨å®šã«é€ã‚‹**
pose_manip.out.link(pose_nn.input)
pose_nn.out.link(xout_pose.input)  # å§¿å‹¢æ¨å®šçµæœã‚’å‡ºåŠ›

# ğŸ® 7ï¸âƒ£ å®Ÿè¡Œ
with dai.Device(pipeline) as device:
    video_q = device.getOutputQueue("rgb", maxSize=4, blocking=False)
    face_q = device.getOutputQueue("face_detection", maxSize=4, blocking=False)
    pose_q = device.getOutputQueue("pose_estimation", maxSize=4, blocking=False)
    pose_config_q = device.getInputQueue("pose_manip_config")

    while True:
        in_video = video_q.get()
        frame = in_video.getCvFrame()

        # ğŸ‘€ é¡”æ¤œå‡ºã®çµæœã‚’å–å¾—ï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æç”»ï¼‰
        in_face = face_q.tryGet()
        if in_face is not None:
            detections = in_face.getFirstLayerFp16()
            if len(detections) > 0:
                for i in range(0, len(detections), 7):
                    conf = detections[i+2]  # ä¿¡é ¼åº¦
                    if conf > 0.5:
                        x1 = detections[i+3]
                        y1 = detections[i+4]
                        x2 = detections[i+5]
                        y2 = detections[i+6]

                        # ğŸ”¥ ã“ã“ã§ã‚¯ãƒ­ãƒƒãƒ—åº§æ¨™ã‚’ `pose_manip` ã«é€ä¿¡
                        cfg = dai.ImageManipConfig()
                        cfg.setCropRect(x1, y1, x2, y2)  
                        cfg.setResize(60, 60)  # ğŸ”¥ ã“ã“ã§ 60x60 ã«ãƒªã‚µã‚¤ã‚º
                        pose_config_q.send(cfg)

                        cv2.rectangle(frame, 
                                      (int(x1 * frame.shape[1]), int(y1 * frame.shape[0])), 
                                      (int(x2 * frame.shape[1]), int(y2 * frame.shape[0])), 
                                      (0, 255, 0), 2)  # ğŸ”¥ é¡”ã‚’æç”»

        # ğŸ‘€ å§¿å‹¢æ¨å®šã®çµæœ
        in_pose = pose_q.tryGet()
        if in_pose is not None:
            yaw = in_pose.getLayerFp16("angle_y_fc")[0]  # ğŸ”¥ ä¿®æ­£
            pitch = in_pose.getLayerFp16("angle_p_fc")[0]  # ğŸ”¥ ä¿®æ­£
            roll = in_pose.getLayerFp16("angle_r_fc")[0]  # ğŸ”¥ ä¿®æ­£

            vals = np.array([abs(pitch), abs(yaw), abs(roll)])
            max_index = np.argmax(vals)

            if vals[max_index] < MIN_THRESHOLD:
                txt = "Face Forward"
            elif max_index == 0:
                txt = "Look down" if pitch > 0 else "Look up"
            elif max_index == 1:
                txt = "Turn right" if yaw > 0 else "Turn left"
            elif max_index == 2:
                txt = "Tilt right" if roll > 0 else "Tilt left"

            cv2.putText(frame, f"{txt} (Yaw: {yaw:.2f}, Pitch: {pitch:.2f}, Roll: {roll:.2f})",
                        (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # ğŸ¥ æ˜ åƒã‚’è¡¨ç¤º
        cv2.imshow("Preview", frame)
        if cv2.waitKey(1) == ord("q"):
            break

cv2.destroyAllWindows()